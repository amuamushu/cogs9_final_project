{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title : Yearly UCSD Application Rate and League Table Linear Regression Model\n",
    "Authors: \n",
    "Thanh Luong\n",
    "Abigail Han\n",
    "Saroop Samra\n",
    "Amy Nguyen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**\n",
    "\n",
    "In this document, include text and code to walk users through your analysis. Feel free to add more cells as you go, but try to keep the five sections provided in order as they are. Feel free to add subsections within each section to organize your project. \n",
    "\n",
    "Be sure to change the title and include authors in the first cell.\n",
    "\n",
    "When finished, download as a PDF and submit the PDF on Gradescope for extra credit. Be sure to include all gorup members. There are up to 15 points extra credit, so even if you don't complete the whole project, completing part of this document will likely earn you some extra credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "Is there a valid SIMPLE linear regression with US News Rankings and Yearly UCSD Application Rate?\n",
    "What does a visualization of this look like?\n",
    "What is the pearson correlation factor?\n",
    "\n",
    "\n",
    "Is there a valid MULTIPLE linear regression with US News Rankings and Times Score and Yearly UCSD Application Rate?\n",
    "What does a visualization of this look like?\n",
    "What is the pearson correlation factor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Include the code to read your dataset in below and briefly explain what information is included in your dataset that you'll use for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-52ee1d89459f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmy_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ec_data.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "my_data = np.loadtxt('ec_data.txt',delimiter=',',dtype=float)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data Sources for attempted Sentimental Analysis</h3>\n",
    "<p> First data source is the training data used to get the probabilities of sentiment given certain words.\n",
    "    The second data source is a News Article on UCSD which we web scraped.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer #for stemming words\n",
    "\n",
    "# training data from CMU's CSE class: http://boston.lti.cs.cmu.edu/classes/95-865-K/HW/HW3/\n",
    "twitter_sent = pd.read_csv(\"twitter_sentiment.csv\")\n",
    "\n",
    "twitter_sent.head()\n",
    "\n",
    "#Splice the words in each column to remove the short words and stem\n",
    "def filter_col(text_col):\n",
    "    spliced = text_col.split(\" \")\n",
    "    filtered = list(filter(remove_short, spliced))\n",
    "    return filtered\n",
    "\n",
    "#remove words that are theoretically too short to mean much, usernames, and hashtags\n",
    "def remove_short(word):\n",
    "    if len(word) <= 3 or \"@\" in word or \"#\" in word:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "#get the stem for same meaning words\n",
    "stemmer = PorterStemmer()\n",
    "filtered_cols = twitter_sent['text'].apply(filter_col)\n",
    "twitter_sent['text'] = filtered_cols.apply(lambda col: [stemmer.stem(word) for word in col])\n",
    "\n",
    "probs = {}\n",
    "\n",
    "row_count = twitter_sent.shape[0]\n",
    "# iterate through all of the rows in the training data\n",
    "for i in range(row_count):\n",
    "    col = twitter_sent['class']\n",
    "    col2 = twitter_sent['text']\n",
    "    emo = str(col[i])\n",
    "    txt = list(col2[i])\n",
    "    \n",
    "    # gets the count of the emotions for every single word in text\n",
    "    for i in range(len(txt)):\n",
    "        spliced = txt[i]\n",
    "        word = probs.get(spliced)\n",
    "        if word:\n",
    "            if word.get(emo):\n",
    "                word[emo] += 1\n",
    "            else:\n",
    "                word[emo] = 1\n",
    "        else:\n",
    "            probs[spliced] = {}\n",
    "            probs[spliced][emo] = 1\n",
    "print(probs[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<p', 'class=\"meta', 'post-meta', 'clearfix\"><span', 'class=\"updated', 'meta-date\"><i', 'class=\"fa', 'fa-calendar\"></i>November', '19,', '2019</span><span', 'class=\"vcard', 'author', 'meta-author\"><span', 'class=\"fn\"><i', 'class=\"fa', 'fa-user\"></i><a', 'href=\"http://triton.news/author/mo/\"', 'rel=\"author\"', 'title=\"Posts', 'by', 'Mo', 'Elew\">Mo', 'Elew</a></span></span><span', 'class=\"meta-tags\"><i', 'class=\"fa', 'fa-tag\"></i><a', 'href=\"http://triton.news/category/news/local/\"', 'rel=\"category', 'tag\">Local</a>,', '<a', 'href=\"http://triton.news/category/news/\"', 'rel=\"category', 'tag\">News</a></span><span', 'class=\"meta-comments\"><i', 'class=\"fa', 'fa-comment-o\"></i><span', 'class=\"dsq-postid\"', 'data-dsqidentifier=\"28835', 'http://triton.news/?p=28835\">0</span></span>\\n</p><p>UC', 'San', 'Diego', 'Political', 'Science', 'Professor', 'Tom', 'Wong', 'plans', 'to', 'announce', 'his', 'campaign', 'for', 'San', 'Diego’s', '53rd', 'Congressional', 'District', 'on', 'November', '20,', 'a', 'seat', 'which', 'opened', 'up', '<a', 'href=\"https://www.sandiegouniontribune.com/news/politics/story/2019-09-04/rep-susan-davis-to-retire-drop-out-of-race-for-reelection-in-2020\">after', 'Democrat', 'Rep.', 'Susan', 'Davis', 'announced</a>', 'in', 'September', 'that', 'she', 'will', 'not', 'seek', 'reelection.', 'Wong', 'will', 'join', 'Sara', 'Jacobs,', '<a', 'href=\"https://www.sandiegouniontribune.com/news/politics/story/2019-09-14/san-diego-council-president-georgette-gomez-kicks-off-congressional-campaign\">San', 'Diego', 'City', 'Council', 'President', 'Georgette', 'Gomez</a>,', 'and', '11', 'others', 'in', 'the', 'race', 'for', 'the', '53rd', 'district.</p><p>Wong', 'submitted', '<a', 'href=\"https://docquery.fec.gov/cgi-bin/forms/C00727461/1362862/\">paperwork</a>', 'to', 'run', 'as', 'a', 'Democrat', 'a', 'few', 'days', 'before', 'his', 'opponent', '<a', 'href=\"https://timesofsandiego.com/politics/2019/11/16/california-democratic-party-endorses-georgette-gomez-in-53rd-congressional-district/\">San', 'Diego', 'Council', 'President', 'Georgette', 'Gómez', 'won', 'the', 'California', 'Democratic', 'Party’s', 'endorsement</a>', 'in', 'the', 'race.</p><p>The', '<a', 'href=\"https://susandavis.house.gov/district/\">53rd', 'Congressional', 'District</a>', 'stretches', 'from', 'the', 'I-5', 'and', 'Balboa', 'Park', 'on', 'the', 'west,', 'through', 'Mission', 'Valley']\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Web scraped an article for text (sentiment analysis)\n",
    "\n",
    "# I consulted these articles:     \n",
    "#       https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/\n",
    "#       https://www.geeksforgeeks.org/reading-selected-webpage-content-using-python-web-scraping/\n",
    "\n",
    "strrr = \"\"\n",
    "req = urllib.request.Request('http://triton.news/2019/11/ucsd-professor-tom-wong-to-announce-run-for-congress-tomorrow/')\n",
    "\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    the_page = response.read()\n",
    "    soup=BeautifulSoup(the_page,'html.parser')\n",
    "    l=soup.findAll(\"p\")\n",
    "    for i in l:\n",
    "        strrr += str(i)\n",
    "    txt_lst = strrr.split(\" \")\n",
    "\n",
    "print(txt_lst[:150])\n",
    "\n",
    "#TO DO: get rid of short words, uses Naive Bayes formula to \n",
    "# compute the probability of negative, positive, or neutral sentiment given words in text (only use words that are\n",
    "# found in the twitter training dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "Include text and code needed to wrangle your data into a tidy data format. If your data are already in a usable format, explain what checks you did on the data (and include the code) to ensure that the data were tidy:\n",
    "\n",
    "Prepare data for simple linear regression : Year, US New Score, Yearly Applicant Rate\n",
    "  We will drop the Number of Applicants, Times Score columns \n",
    "  \n",
    "Prepare data for multiple linear regression : Year, US New Score, Times Score, Yearly Applicant Rate\n",
    "  We will drop the Number of Applicants\n",
    "  We will drop all Rows where the Times Score is zero (data was unavailable)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Linear Regression Data\n",
      "[[43.    0.  ]\n",
      " [34.    5.96]\n",
      " [33.   11.92]\n",
      " [32.   15.85]\n",
      " [32.    9.69]\n",
      " [31.    6.99]\n",
      " [31.    8.27]\n",
      " [31.    5.07]\n",
      " [32.   -4.42]\n",
      " [35.   -2.38]\n",
      " [32.    7.54]\n",
      " [38.    3.43]\n",
      " [38.    5.09]\n",
      " [35.   -0.69]\n",
      " [35.    2.21]\n",
      " [35.   11.13]\n",
      " [37.   13.81]\n",
      " [38.   10.78]\n",
      " [39.    8.99]\n",
      " [37.    6.28]\n",
      " [39.    7.87]\n",
      " [44.    5.04]\n",
      " [42.   10.68]]\n",
      "Multiple Linear Regression Data\n",
      "[[35.   73.2  11.13]\n",
      " [37.   73.   13.81]\n",
      " [38.   75.2  10.78]\n",
      " [39.   67.4   8.99]\n",
      " [37.   68.6   6.28]\n",
      " [39.   72.2   7.87]\n",
      " [44.   73.2   5.04]\n",
      " [42.   78.7  10.68]]\n"
     ]
    }
   ],
   "source": [
    "# Simple Linear Regression Data: Drop the Year, Number of Applicants, Times Score columns\n",
    "simple_linear_regress_data = np.delete(my_data, [0, 2,3], 1)\n",
    "print(\"Simple Linear Regression Data\")\n",
    "print(simple_linear_regress_data)\n",
    "\n",
    "# Multiple Linear Regression Data: Drop the Year, Number of Applicant columns\n",
    "#                                  Drop All Rows where Times Score is Zero\n",
    "multiple_linear_regress_data = np.delete(my_data, [0, 3], 1)\n",
    "multiple_linear_regress_data = multiple_linear_regress_data[multiple_linear_regress_data[:,1] != 0.0]\n",
    "print(\"Multiple Linear Regression Data\")\n",
    "print(multiple_linear_regress_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "Include text and code to explore and analyze the data your group used for your final project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'simple_linear_regress_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1813d6f945bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_linear_regress_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_linear_regress_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'simple_linear_regress_data' is not defined"
     ]
    }
   ],
   "source": [
    "X = simple_linear_regress_data[:,0]\n",
    "Y = simple_linear_regress_data[:,1]\n",
    "results = sm.OLS(Y,sm.add_constant(X)).fit()\n",
    "print(results.summary())\n",
    "plt.scatter(X, Y, s=np.pi*3)\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.xlabel('US News Score')\n",
    "plt.ylabel('UCSD Yearly Applicant Rate')\n",
    "plt.plot(np.unique(X), np.poly1d(np.polyfit(X, Y, 1))(np.unique(X)))\n",
    "plt.show()\n",
    "\n",
    "X = np.delete(multiple_linear_regress_data, [2], 1)\n",
    "Y = multiple_linear_regress_data[:,2]\n",
    "print(X)\n",
    "results = sm.OLS(Y,sm.add_constant(X)).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Experience\n",
    "\n",
    "In this section, include information about your team's level of exprience programming in Python. Discuss briefly what new things (concepts, skills, packages, etc.) you learned by completing this extra credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h>Three of the members have had Python programming experiance and are Data Science Majors (2 sophmores and 1 freshman). Some of us have taken DSC10 and DSC20 courses. However, we all needed refreshes as it was a while ago that some of us last wrote Python We also learned the new API: statsmodels. We didnt use this in our prior classes. We used this to perform simple and multiple linear regression. We also did lots of Google searches, this was useful to learn how to plot a scatter chart.</h>\n",
    "\n",
    "<p>Amy's Past Experiences: I took DSC 20 and did a did a Data Challenge so I have basic experience using Python and Pandas.<br> Amy's contributions: I did the attempt for the sentiment analysis portion so I learned a lot about having reading data from a URL using URLlib and using beautiful soup to get rid of the unneccesary HTML. I applied the concepts of tokenization to stem words and remove unneccesary words for analysis. This is my first time using PorterStemmer. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
